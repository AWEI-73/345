import streamlit as st
import yfinance as yf
import vertexai
from vertexai.generative_models import GenerativeModel
import logging
import traceback
from typing import Dict, Optional, List, Tuple
import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
import feedparser
from datetime import datetime
import re
import random
import os

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Vertex AI è¨­å®š
PROJECT_ID = os.getenv("PROJECT_ID", "ageless-courier-460814-p0")
LOCATION = os.getenv("LOCATION", "us-central1")
MODEL_ID = os.getenv("MODEL_ID", "gemini-2.5-flash-preview-05-20")

class PTTScraper:
    def __init__(self, max_retries: int = 3, retry_delay: int = 5):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.cookies = {'over18': '1'}

    def _make_request(self, url: str) -> Optional[requests.Response]:
        """ç™¼é€è«‹æ±‚ä¸¦è™•ç†é‡è©¦é‚è¼¯"""
        for attempt in range(self.max_retries):
            try:
                response = self.session.get(url, cookies=self.cookies, timeout=10)
                response.raise_for_status()
                return response
            except requests.exceptions.RequestException as e:
                logger.warning(f"è«‹æ±‚å¤±æ•— (å˜—è©¦ {attempt + 1}/{self.max_retries}): {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(self.retry_delay + random.uniform(1, 3))
                continue
        return None

    def get_ptt_stock_posts(self, url: str = "https://www.ptt.cc/bbs/Stock/index.html", 
                           num_posts: int = 5) -> List[Dict]:
        """ç²å– PTT Stock ç‰ˆæ–‡ç« åˆ—è¡¨"""
        try:
            response = self._make_request(url)
            if not response:
                return []

            soup = BeautifulSoup(response.text, 'html.parser')
            posts_data = []

            for post_div in soup.find_all('div', class_='r-ent')[:num_posts]:
                try:
                    post_info = self._extract_post_info(post_div)
                    if post_info:
                        # ç²å–æ–‡ç« å…§å®¹
                        content = self._get_post_content(post_info['link'])
                        post_info['content'] = content
                        posts_data.append(post_info)
                except Exception as e:
                    logger.error(f"è™•ç†æ–‡ç« æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                    continue

            return posts_data
        except Exception as e:
            logger.error(f"çˆ¬å– PTT æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return []

    def _extract_post_info(self, post_div) -> Optional[Dict]:
        """å¾æ–‡ç«  div ä¸­æå–è³‡è¨Š"""
        try:
            title_tag = post_div.find('div', class_='title')
            if not title_tag or not title_tag.a:
                return None

            title = title_tag.a.text.strip()
            link = "https://www.ptt.cc" + title_tag.a['href']
            
            author_tag = post_div.find('div', class_='author')
            author = author_tag.text.strip() if author_tag else "N/A"
            
            date_tag = post_div.find('div', class_='date')
            date = date_tag.text.strip() if date_tag else "N/A"

            # æ¸…ç†å’Œé©—è­‰è³‡æ–™
            title = self._clean_text(title)
            author = self._clean_text(author)
            date = self._clean_text(date)

            return {
                'title': title,
                'link': link,
                'author': author,
                'date': date
            }
        except Exception as e:
            logger.error(f"æå–æ–‡ç« è³‡è¨Šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None

    def _get_post_content(self, url: str) -> str:
        """ç²å–æ–‡ç« å…§å®¹"""
        try:
            response = self._make_request(url)
            if not response:
                return "ç„¡æ³•ç²å–æ–‡ç« å…§å®¹"

            soup = BeautifulSoup(response.text, 'html.parser')
            content_div = soup.find(id='main-content')
            
            if not content_div:
                return "ç„¡æ³•æ‰¾åˆ°æ–‡ç« å…§å®¹"

            # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
            for element in content_div.find_all(['div', 'span']):
                element.decompose()

            content = content_div.text.strip()
            return self._clean_text(content)
        except Exception as e:
            logger.error(f"ç²å–æ–‡ç« å…§å®¹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return "ç²å–æ–‡ç« å…§å®¹æ™‚ç™¼ç”ŸéŒ¯èª¤"

    @staticmethod
    def _clean_text(text: str) -> str:
        """æ¸…ç†æ–‡å­—å…§å®¹"""
        # ç§»é™¤å¤šé¤˜çš„ç©ºç™½
        text = re.sub(r'\s+', ' ', text)
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[^\w\s\u4e00-\u9fff.,!?ï¼Œã€‚ï¼ï¼Ÿ]', '', text)
        return text.strip()

def get_rss_news(num_items: int = 3) -> List[Dict]:
    """å¾ Yahoo å¥‡æ‘©è‚¡å¸‚ RSS ç²å–æ–°è"""
    try:
        # Yahoo å¥‡æ‘©è‚¡å¸‚çš„å°è‚¡å‹•æ…‹ RSS
        url = "https://tw.stock.yahoo.com/rss"
        feed = feedparser.parse(url)
        
        news_items = []
        for entry in feed.entries[:num_items]:
            news_items.append({
                'title': entry.title,
                'summary': entry.summary if hasattr(entry, 'summary') else '',
                'link': entry.link,
                'published': entry.published if hasattr(entry, 'published') else ''
            })
        return news_items
    except Exception as e:
        logger.error(f"ç²å– RSS æ–°èæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return []

def fetch_twse_stock_list() -> Dict[str, str]:
    """å¾å°ç£è­‰åˆ¸äº¤æ˜“æ‰€ç¶²ç«™ç²å–ä¸Šå¸‚å…¬å¸æ¸…å–®"""
    try:
        url = "https://isin.twse.com.tw/isin/class_main.jsp?owncode=&stockname=&isincode=&market=1&issuetype=1&industry_code=&Page=1&chklike=Y"
        response = requests.get(url)
        response.encoding = 'ms950'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        stock_dict = {}
        for row in soup.find_all('tr')[1:]:  # è·³éæ¨™é¡Œè¡Œ
            cols = row.find_all('td')
            if len(cols) >= 4:
                code = cols[2].text.strip()
                name = cols[3].text.strip()
                if code and name and code.isdigit():
                    stock_dict[name] = code
        
        return stock_dict
    except Exception as e:
        logger.error(f"ç²å–ä¸Šå¸‚å…¬å¸æ¸…å–®æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return {}

# åˆå§‹åŒ– Vertex AI
try:
    print(f"\n[1] æ­£åœ¨åˆå§‹åŒ– Vertex AI...")
    vertexai.init(project=PROJECT_ID, location=LOCATION)
    print(f"[SUCCESS] Vertex AI åˆå§‹åŒ–æˆåŠŸã€‚")

    print(f"\n[2] æ­£åœ¨è¼‰å…¥æ¨¡å‹: {MODEL_ID}...")
    model = GenerativeModel(MODEL_ID)
    print(f"[SUCCESS] æ¨¡å‹è¼‰å…¥æˆåŠŸã€‚")
except Exception as e:
    st.error(f"Vertex AI åˆå§‹åŒ–å¤±æ•—: {e}")
    st.stop()

class StockAnalyzer:
    def __init__(self):
        self.model = model
        self.tw_stocks = fetch_twse_stock_list()
        self.ptt_scraper = PTTScraper()
        logger.info(f"å·²è¼‰å…¥ {len(self.tw_stocks)} å®¶ä¸Šå¸‚å…¬å¸è³‡æ–™")

    def search_stocks(self, query: str) -> List[Tuple[str, str]]:
        """æœå°‹è‚¡ç¥¨"""
        if not query:
            return []
        
        # æœå°‹å…¬å¸åç¨±æˆ–ä»£è™Ÿ
        results = []
        for name, code in self.tw_stocks.items():
            if query.lower() in name.lower() or query in code:
                results.append((name, code))
        return results

    def get_stock_info(self, ticker: str) -> Dict:
        """ç²å–è‚¡ç¥¨è³‡è¨Š"""
        try:
            # ç¢ºä¿è‚¡ç¥¨ä»£è™Ÿæ ¼å¼æ­£ç¢º
            if not ticker.endswith('.TW') and not ticker.endswith('.TWO'):
                ticker = f"{ticker}.TW"
            
            stock = yf.Ticker(ticker)
            info = stock.info
            
            # æª¢æŸ¥æ˜¯å¦æˆåŠŸç²å–è³‡è¨Š
            if not info:
                return {"error": f"ç„¡æ³•ç²å–è‚¡ç¥¨ {ticker} çš„è³‡è¨Š"}
            
            # ç²å–æ­·å²è³‡æ–™
            hist = stock.history(period="5d")
            
            # æ ¼å¼åŒ–æ•¸å€¼
            if 'marketCap' in info and info['marketCap']:
                info['marketCap'] = f"{info['marketCap']/100000000:.2f}å„„"
            
            return {
                "info": info,
                "history": hist if not hist.empty else None
            }
        except Exception as e:
            logger.error(f"æŸ¥è©¢è‚¡ç¥¨ {ticker} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            traceback.print_exc()
            return {"error": f"æŸ¥è©¢è‚¡ç¥¨ {ticker} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"}

    def ask_gemini_for_term(self, term: str) -> Dict:
        """ä½¿ç”¨ Gemini è§£é‡‹é‡‘èåè©"""
        prompt = f"""è§’è‰²ï¼šä½ æ˜¯ä¸€ä½ç²¾é€šé‡‘èçš„AIåŠ©ç†ã€‚
ä»»å‹™ï¼šè«‹ç”¨ç¹é«”ä¸­æ–‡å‘ä¸€ä½é‡‘èæ–°æ‰‹è§£é‡‹ã€Œ{term}ã€æ˜¯ä»€éº¼ã€‚
è«‹ç›¡é‡ä½¿ç”¨ç”Ÿæ´»åŒ–çš„æ¯”å–»ï¼Œä¸¦èªªæ˜å…¶é‡è¦æ€§ã€‚é¿å…æä¾›ä»»ä½•æŠ•è³‡å»ºè­°ã€‚

è«‹åš´æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¼¸å‡ºï¼ˆä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å…§å®¹ï¼‰ï¼š
åè©è§£é‡‹ï¼š[æ­¤è™•å¡«å¯«åè©è§£é‡‹]
é‡è¦æ€§ï¼š[æ­¤è™•å¡«å¯«é‡è¦æ€§èªªæ˜]
ç”Ÿæ´»æ¯”å–»ï¼š[æ­¤è™•å¡«å¯«ç”Ÿæ´»åŒ–æ¯”å–»]"""
        try:
            response = self.model.generate_content(prompt)
            
            # è§£æ Gemini çš„å›æ‡‰
            lines = response.text.split('\n')
            analysis_result = {}
            for line in lines:
                if line.startswith("åè©è§£é‡‹ï¼š"):
                    analysis_result['definition'] = line.replace("åè©è§£é‡‹ï¼š", "").strip()
                elif line.startswith("é‡è¦æ€§ï¼š"):
                    analysis_result['importance'] = line.replace("é‡è¦æ€§ï¼š", "").strip()
                elif line.startswith("ç”Ÿæ´»æ¯”å–»ï¼š"):
                    analysis_result['analogy'] = line.replace("ç”Ÿæ´»æ¯”å–»ï¼š", "").strip()
            return analysis_result
        except Exception as e:
            logger.error(f"Gemini è§£é‡‹ã€Œ{term}ã€æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            traceback.print_exc()
            return {"error": f"Gemini è§£é‡‹ã€Œ{term}ã€æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"}

    def analyze_news_with_gemini(self, title: str, summary: str) -> Dict:
        """ä½¿ç”¨ Gemini åˆ†ææ–°è"""
        prompt = f"""è§’è‰²ï¼šä½ æ˜¯ä¸€ä½è³‡æ·±è²¡ç¶“è¨˜è€…ï¼Œä¹Ÿæ˜¯ä¸€ä½æ“…é•·å‘æ–°æ‰‹è§£é‡‹è‚¡å¸‚çš„å°å¸«ã€‚
ä»»å‹™ï¼šè«‹é–±è®€ä»¥ä¸‹è²¡ç¶“æ–°èæ¨™é¡Œèˆ‡æ‘˜è¦ï¼Œä¸¦å®Œæˆä¸‹åˆ—ä»»å‹™ï¼š
1. åˆ¤æ–·æ­¤æ–°èå°ç›¸é—œè‚¡ç¥¨çš„æ½›åœ¨å½±éŸ¿æ˜¯åã€Œåˆ©å¤šã€ã€ã€Œåˆ©ç©ºã€ï¼Œé‚„æ˜¯ã€Œä¸­æ€§ã€ã€‚
2. é‡å°é€™å‰‡æ–°èï¼Œæ’°å¯«ä¸€æ®µç´„50-100å­—çš„ã€Œæ–°æ‰‹çœ‹æ³•è§£é‡‹ã€ï¼Œèªªæ˜é€™å‰‡æ–°èç‚ºä»€éº¼è¢«èªç‚ºæ˜¯åˆ©å¤š/åˆ©ç©º/ä¸­æ€§ï¼Œä»¥åŠå®ƒé€šå¸¸å¯èƒ½å°è‚¡åƒ¹ç”¢ç”Ÿä»€éº¼æ¨£çš„åˆæ­¥å½±éŸ¿ã€‚è«‹é¿å…ä½¿ç”¨éæ–¼å°ˆæ¥­çš„è¡“èªï¼Œä¸¦å¼·èª¿é€™ä¸æ˜¯æŠ•è³‡å»ºè­°ã€‚

æ–°èæ¨™é¡Œï¼š{title}
æ–°èæ‘˜è¦ï¼š{summary}

è«‹åš´æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¼¸å‡ºï¼ˆä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å…§å®¹ï¼‰ï¼š
æƒ…æ„Ÿåˆ¤æ–·ï¼š[åˆ©å¤š/åˆ©ç©º/ä¸­æ€§]
æ–°æ‰‹çœ‹æ³•ï¼š[æ­¤è™•å¡«å¯«æ–°æ‰‹çœ‹æ³•è§£é‡‹]"""
        try:
            response = self.model.generate_content(prompt)
            
            # è§£æ Gemini çš„å›æ‡‰
            lines = response.text.split('\n')
            analysis_result = {}
            for line in lines:
                if line.startswith("æƒ…æ„Ÿåˆ¤æ–·ï¼š"):
                    analysis_result['sentiment'] = line.replace("æƒ…æ„Ÿåˆ¤æ–·ï¼š", "").strip()
                elif line.startswith("æ–°æ‰‹çœ‹æ³•ï¼š"):
                    analysis_result['novice_explanation'] = line.replace("æ–°æ‰‹çœ‹æ³•ï¼š", "").strip()
            return analysis_result
        except Exception as e:
            logger.error(f"Gemini åˆ†ææ–°èæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            traceback.print_exc()
            return {"error": f"Gemini åˆ†ææ–°èæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"}

    def summarize_ptt_post(self, content: str, num_sentences: int = 3) -> str:
        """ä½¿ç”¨ Gemini é€²è¡Œ PTT æ–‡ç« æ‘˜è¦"""
        prompt = f"""
        è§’è‰²ï¼šä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„å…§å®¹ç·¨è¼¯ã€‚
        ä»»å‹™ï¼šè«‹å°‡ä»¥ä¸‹æä¾›çš„æ–‡æœ¬å…§å®¹ï¼Œç”¨ç¹é«”ä¸­æ–‡æ‘˜è¦æˆ {num_sentences} å¥è©±çš„é‡é»ã€‚

        æ–‡æœ¬å…§å®¹ï¼š
        ---
        {content}
        ---
        æ‘˜è¦ï¼š
        """
        try:
            response = self.model.generate_content(prompt)
            return response.text
        except Exception as e:
            logger.error(f"Gemini æ‘˜è¦æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            traceback.print_exc()
            return f"æ‘˜è¦ç”Ÿæˆå¤±æ•—: {str(e)}"

# --- Streamlit UI ---
st.set_page_config(page_title="å°è‚¡ç¥åŠ©ç† (Geminiç‰ˆ)", layout="wide")
st.title("å°è‚¡ç¥åŠ©ç† (Gemini API å¯¦æˆ°ç‰ˆ)")

# åˆå§‹åŒ–åˆ†æå™¨
analyzer = StockAnalyzer()

# åŠŸèƒ½1: è§£é‡‹é‡‘èåè©
st.header("ğŸ“š é‡‘èåè©è§£é‡‹")
term_input = st.text_input("è¼¸å…¥ä½ æƒ³æŸ¥è©¢çš„é‡‘èåè© (ä¾‹å¦‚ï¼šEPSã€æ®–åˆ©ç‡)ï¼š")
if st.button("æŸ¥è©¢åè©è§£é‡‹"):
    if term_input:
        with st.spinner("Gemini æ­£åœ¨æ€è€ƒä¸­..."):
            result = analyzer.ask_gemini_for_term(term_input)
            
        if 'error' in result:
            st.error(result['error'])
        else:
            st.markdown("### åè©è§£é‡‹")
            st.write(result.get('definition', 'N/A'))
            st.markdown("### é‡è¦æ€§")
            st.write(result.get('importance', 'N/A'))
            st.markdown("### ç”Ÿæ´»æ¯”å–»")
            st.write(result.get('analogy', 'N/A'))
    else:
        st.warning("è«‹è¼¸å…¥é‡‘èåè©ã€‚")

st.divider()

# åŠŸèƒ½2: æŸ¥è©¢è‚¡ç¥¨è³‡è¨Š
st.header("ğŸ“ˆ è‚¡ç¥¨åŸºæœ¬è³‡è¨ŠæŸ¥è©¢")

# ä½¿ç”¨ session_state ä¾†ä¿å­˜é¸æ“‡çš„è‚¡ç¥¨ä»£è™Ÿ
if 'selected_ticker' not in st.session_state:
    st.session_state.selected_ticker = ""
if 'show_results' not in st.session_state:
    st.session_state.show_results = False

# æœå°‹è‚¡ç¥¨
search_query = st.text_input("æœå°‹å…¬å¸åç¨±æˆ–è‚¡ç¥¨ä»£è™Ÿï¼š")
if search_query:
    search_results = analyzer.search_stocks(search_query)
    if search_results:
        st.write("æœå°‹çµæœï¼š")
        for name, code in search_results:
            if st.button(f"{name} ({code})", key=f"btn_{code}"):
                st.session_state.selected_ticker = code
                st.session_state.show_results = True
                st.rerun()
    else:
        st.warning("æ‰¾ä¸åˆ°ç¬¦åˆçš„è‚¡ç¥¨ï¼Œè«‹å˜—è©¦å…¶ä»–é—œéµå­—ã€‚")

# ç›´æ¥è¼¸å…¥è‚¡ç¥¨ä»£è™Ÿ
direct_input = st.text_input("æˆ–ç›´æ¥è¼¸å…¥è‚¡ç¥¨ä»£è™Ÿ (ä¾‹å¦‚ï¼š2330)ï¼š")

# æŸ¥è©¢æŒ‰éˆ•
if st.button("æŸ¥è©¢è‚¡ç¥¨è³‡è¨Š") or st.session_state.show_results:
    # å„ªå…ˆä½¿ç”¨æœå°‹çµæœé¸æ“‡çš„è‚¡ç¥¨ä»£è™Ÿ
    ticker_to_query = st.session_state.selected_ticker if st.session_state.selected_ticker else direct_input
    
    if ticker_to_query:
        with st.spinner(f"æ­£åœ¨æŸ¥è©¢ {ticker_to_query} çš„è³‡è¨Š..."):
            try:
                result = analyzer.get_stock_info(ticker_to_query)
                
                if 'error' in result:
                    st.error(result['error'])
                else:
                    info = result['info']
                    hist = result['history']

                    col1, col2 = st.columns(2)
                    with col1:
                        st.subheader(f"{info.get('longName', ticker_to_query)} ({info.get('symbol', '')})")
                        current_price = info.get('regularMarketPrice')
                        if current_price:
                            st.metric(
                                label="ç›®å‰è‚¡åƒ¹", 
                                value=f"{current_price:.2f} {info.get('currency', '')}",
                                delta=f"{info.get('regularMarketChange', 0):.2f} ({info.get('regularMarketChangePercent', 0)*100:.2f}%)"
                            )
                        else:
                            st.write("ç›®å‰è‚¡åƒ¹ï¼šç„¡æ³•å–å¾—")

                    with col2:
                        st.write(f"**ç”¢æ¥­ï¼š** {info.get('sector', 'N/A')}")
                        st.write(f"**å¸‚å€¼ï¼š** {info.get('marketCap', 'N/A')}")
                        st.write(f"**ç¶²ç«™ï¼š** {info.get('website', 'N/A')}")

                    # é¡¯ç¤ºæœ€è¿‘äº”æ—¥æ­·å²è‚¡åƒ¹åœ–è¡¨
                    if hist is not None and not hist.empty:
                        st.subheader("æœ€è¿‘äº”æ—¥è‚¡åƒ¹")
                        st.line_chart(hist['Close'])
                    else:
                        st.warning("ç„¡æ³•ç²å–è‚¡åƒ¹æ­·å²è³‡æ–™ã€‚")
            except Exception as e:
                st.error(f"æŸ¥è©¢è‚¡ç¥¨è³‡è¨Šæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
                logger.error(f"æŸ¥è©¢è‚¡ç¥¨ {ticker_to_query} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                traceback.print_exc()
    else:
        st.warning("è«‹è¼¸å…¥è‚¡ç¥¨ä»£è™Ÿæˆ–æœå°‹å…¬å¸åç¨±ã€‚")

# é‡ç½®é¸æ“‡çš„è‚¡ç¥¨ä»£è™Ÿ
if st.button("æ¸…é™¤é¸æ“‡"):
    st.session_state.selected_ticker = ""
    st.session_state.show_results = False
    st.rerun()

st.divider()

# åŠŸèƒ½3: æ¯æ—¥å¸‚å ´è§€å¯Ÿ
st.header("â˜€ï¸ æ¯æ—¥å¸‚å ´è§€å¯Ÿ")

if st.button("ç”¢ç”Ÿä»Šæ—¥è§€å¯Ÿå ±å‘Š"):
    with st.spinner("æ­£åœ¨ç”¢ç”Ÿæ¯æ—¥è§€å¯Ÿå ±å‘Š..."):
        # 1. TAIEX æŒ‡æ•¸æ‘˜è¦
        try:
            taiex = yf.Ticker("^TWII")
            taiex_hist = taiex.history(period="1d")
            if not taiex_hist.empty:
                latest_taiex = taiex_hist.iloc[-1]
                taiex_info = (
                    f"å°ç£åŠ æ¬ŠæŒ‡æ•¸ ({latest_taiex.name.strftime('%Y-%m-%d')}): "
                    f"æ”¶ç›¤ {latest_taiex['Close']:.2f}, "
                    f"æ¼²è·Œ {latest_taiex['Close'] - latest_taiex['Open']:.2f} "
                    f"(é–‹ç›¤: {latest_taiex['Open']:.2f}, æœ€é«˜: {latest_taiex['High']:.2f}, æœ€ä½: {latest_taiex['Low']:.2f})"
                )
                st.subheader("ğŸ“Š ä»Šæ—¥å°è‚¡æŒ‡æ•¸è®ŠåŒ–æ‘˜è¦")
                st.write(taiex_info)
            else:
                st.warning("ç„¡æ³•ç²å– TAIEX æŒ‡æ•¸è³‡æ–™ã€‚")
        except Exception as e:
            st.error(f"ç²å– TAIEX è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

        # 2. å¾ RSS ç²å–æ–°èä¸¦åˆ†æ
        st.subheader("ğŸ“° ä»Šæ—¥é‡é»æ–°èåˆ†æ")
        daily_news = get_rss_news(num_items=3)
        if daily_news:
            for item in daily_news:
                st.markdown(f"**æ¨™é¡Œï¼š** [{item['title']}]({item['link']})")
                analysis = analyzer.analyze_news_with_gemini(item['title'], item['summary'])
                if 'error' in analysis:
                    st.error(f"  åˆ†æéŒ¯èª¤: {analysis['error']}")
                else:
                    st.info(f"  **Gemini æƒ…æ„Ÿåˆ¤æ–·ï¼š** {analysis.get('sentiment', 'N/A')}")
                    st.caption(f"  **Gemini æ–°æ‰‹çœ‹æ³•ï¼š** {analysis.get('novice_explanation', 'N/A')}")
                    st.markdown("---")
        else:
            st.warning("ä»Šæ—¥ç„¡è¶³å¤ æ–°èå¯ä¾›åˆ†æã€‚")

        st.success("æ¯æ—¥è§€å¯Ÿå ±å‘Šç”¢ç”Ÿå®Œç•¢ï¼")

st.divider()

# åŠŸèƒ½4: PTT è‚¡ç¥¨ç‰ˆæ–‡ç« 
st.header("ğŸ’¬ PTT è‚¡ç¥¨ç‰ˆç†±é–€æ–‡ç« ")

if st.button("æ›´æ–° PTT æ–‡ç« "):
    with st.spinner("æ­£åœ¨ç²å– PTT è‚¡ç¥¨ç‰ˆæ–‡ç« ..."):
        try:
            ptt_posts = analyzer.ptt_scraper.get_ptt_stock_posts()
            if ptt_posts:
                for post in ptt_posts:
                    with st.expander(f"{post['title']} (ä½œè€…: {post['author']}, æ—¥æœŸ: {post['date']})"):
                        st.markdown(f"[åŸæ–‡é€£çµ]({post['link']})")
                        st.markdown("**æ–‡ç« æ‘˜è¦ï¼š**")
                        summary = analyzer.summarize_ptt_post(post['content'])
                        st.write(summary)
                        st.markdown("---")
            else:
                st.warning("ç„¡æ³•ç²å– PTT æ–‡ç« ã€‚")
        except Exception as e:
            st.error(f"ç²å– PTT æ–‡ç« æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            logger.error(f"ç²å– PTT æ–‡ç« æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            traceback.print_exc()

# é‹è¡Œ Streamlit App: åœ¨çµ‚ç«¯æ©Ÿä¸­è¼¸å…¥ `streamlit run your_script_name.py`
